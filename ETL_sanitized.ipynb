{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import traceback  # [QA CHANGE] For logging full tracebacks\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pyzotero #(has a Squiggly orange line under it)\n",
    "# BioPython for PubMed\n",
    "from Bio import Entrez\n",
    "\n",
    "# PyAlex for OpenAlex\n",
    "import pyalex #(has a Squiggly orange line under it)\n",
    "from pyalex import Works #(has a Squiggly white line under it)\n",
    "\n",
    "# Azure\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "\n",
    "# PyZotero for Zotero API access\n",
    "from pyzotero import zotero #(has a Squiggly white line under \"pyzotero\", and \"zotero\" is white text)\n",
    "\n",
    "# citeproc-py for citation formatting\n",
    "from citeproc import CitationStylesStyle, CitationStylesBibliography, formatter #(has a Squiggly white line under \"citeproc\" and is white text)\n",
    "from citeproc import Citation, CitationItem #(has a Squiggly white line under \"citeproc\" and is white text )\n",
    "from citeproc.source.json import CiteProcJSON #(has a Squiggly white line under \"citeproc\" and is white text)\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For approximate token counting (optional)\n",
    "try:\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    def count_tokens(text: str) -> int:\n",
    "        return len(tokenizer.encode(text)) if isinstance(text, str) else 0\n",
    "except ImportError:\n",
    "    tokenizer = None\n",
    "    def count_tokens(text: str) -> int:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CONFIG ==============\n",
    "LOG_DIR = \"G:/ETL_LOGS/pipeline_test.log\"\n",
    "os.makedirs(LOG_DIR, exist_ok=True)  # [QA CHANGE] Ensure directory exists\n",
    "\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"pipeline_test.log\")\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    filemode='w',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# [QA CHANGE] Optional: Also log CRITICAL errors to console if you wish\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.ERROR)\n",
    "formatter_console = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter_console)\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\n",
    "#--------- configure API's -------------\n",
    "Entrez.email = os.getenv(\"ENTREZ_EMAIL\", \"default_email@example.com\")\n",
    "UNPAYWALL_EMAIL = os.getenv(\"UNPAYWALL_EMAIL\", \"default_email@example.com\")\n",
    "pyalex.config.email = os.getenv(\"PYALEX_EMAIL\", \"default_email@example.com\")\n",
    "\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT\", \"https://your-azure-endpoint.cognitiveservices.azure.com/\")\n",
    "AZURE_KEY = os.getenv(\"AZURE_KEY\", \"your azure key\")\n",
    "\n",
    "if not AZURE_KEY or \"YOUR_KEY\" in AZURE_KEY:\n",
    "    logging.error(\"AZURE_KEY is missing or placeholder. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "#-----------Directories----------------\n",
    "OUTPUT_CSV = os.getenv(\"OUTPUT_CSV\", \"G:/ETL_OUT/ETL_output.csv\")\n",
    "OUTPUT_FEATHER = os.getenv(\"OUTPUT_FEATHER\", \"G:/ETL_OUT/ETL_output2.feather\")\n",
    "ENRICHED_FEATHER = os.getenv(\"ENRICHED_FEATHER\", \"G:/ETL_OUT/enriched_output.feather\")\n",
    "PDF_SAVE_FOLDER = os.getenv(\"PDF_SAVE_FOLDER\", \"G:/ETL_PDFS\")\n",
    "\n",
    "os.makedirs(PDF_SAVE_FOLDER, exist_ok=True) # [QA CHANGE] Ensure directory exists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Zotero Configuration ----------\n",
    "ZOTERO_LIBRARY_ID = os.getenv(\"ZOTERO_LIBRARY_ID\", \"your zotero library ID\")\n",
    "ZOTERO_LIBRARY_TYPE = os.getenv(\"ZOTERO_LIBRARY_TYPE\", \"user\")\n",
    "ZOTERO_API_KEY = os.getenv(\"ZOTERO_API_KEY\", \"your zotero API key\")\n",
    "zot = zotero.Zotero(ZOTERO_LIBRARY_ID, ZOTERO_LIBRARY_TYPE, ZOTERO_API_KEY)\n",
    "\n",
    "# Default collection key for added PDFs\n",
    "DEFAULT_ZOTERO_COLLECTION = os.getenv(\"DEFAULT_ZOTERO_COLLECTION\", \"your zotero collection\")\n",
    "\n",
    "# [QA CHANGE] Validate the Zotero credentials\n",
    "if not ZOTERO_LIBRARY_ID or \"YOUR_LIBRARY_ID\" in ZOTERO_LIBRARY_ID:\n",
    "    logging.error(\"ZOTERO_LIBRARY_ID is missing or placeholder. Exiting.\")\n",
    "    sys.exit(1)\n",
    "if not ZOTERO_API_KEY:\n",
    "    logging.error(\"ZOTERO_API_KEY is missing. Exiting.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZOTERO_API_KEY = \"your zotero key\"\n",
    "# Try to fetch key info to verify connectivity\n",
    "try:\n",
    "    key_info = zot.key_info()\n",
    "    print(\"Zotero API connection successful. Key info:\")\n",
    "    print(key_info)\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to Zotero:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ChatGPT Setup\n",
    "# -----------------------------\n",
    "OPEN_API_KEY =\"your OpenAI API key\"\n",
    "openai.api_key = OPEN_API_KEY\n",
    "logging.debug(\"OpenAI API key set.\")\n",
    "MODEL_NAME = \"text-embedding-3-large\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary of user-friendly style names => .csl file paths\n",
    "# NOTE: you have to download these files: https://www.zotero.org/styles\n",
    "# Havent tested all styles, style \"2\" works for sure\n",
    "# ------------------------------------------------------------------------------\n",
    "CITATION_STYLES = {\n",
    "    \"1\": {\"name\": \"American Medical Association 11th edition\",\"path\": r\"G:\\ETL_CSL\\american-medical-association.csl\"},\n",
    "    \"2\": {\"name\": \"American Psychological Association (APA) 6th Edition \",\"path\": r\"G:\\ETL_CSL\\apa-6th-edition.csl\"},\n",
    "    \"3\": {\"name\": \"American Psychological Association (APA) 7th Edition \",\"path\": r\"G:\\ETL_CSL\\apa.csl\"},\n",
    "    \"4\": {\"name\": \"American Psychological Association (APA) 7th Edition (annotated bibliography)\",\"path\": r\"G:\\ETL_CSL\\apa-annotated-bibliography_7th.csl\"},\n",
    "    \"5\": {\"name\": \"American Political Science Association\",\"path\": r\"G:\\ETL_CSL\\APSA.csl\"},\n",
    "    \"6\": {\"name\": \"Chicago Manual of Style 17th Edition (author-date)\",\"path\": r\"G:\\ETL_CSL\\CHICAGO_AUTHOR_DATE.csl\"},\n",
    "    \"7\": {\"name\": \"Chicago Manual of Style 17th Edition (full note)\", \"path\": r\"G:\\ETL_CSL\\chicago-fullnote-bibliography.csl\"},\n",
    "    \"8\": {\"name\": \"Chicago Manual of Style 17th edition (note)\", \"path\": r\"G:\\ETL_CSL\\chicago-note-bibliography.csl\"},\n",
    "    \"9\": {\"name\": \"Cite Them Right 12th edition - Harvard\", \"path\": r\"G:\\ETL_CSL\\harvard-cite-them-right.csl\"},\n",
    "    \"10\": {\"name\": \"Elsevier - Harvard (with titles)\",\"path\": r\"G:\\ETL_CSL\\elsevier-harvard.csl\"},\n",
    "    \"11\": {\"name\": \"IEEE\",\"path\": r\"G:\\ETL_CSL\\ieee.csl\"},\n",
    "    \"12\": {\"name\": \"Modern Humanities Research Association 4th edition (notes with bibliography)\",\"path\": r\"G:\\ETL_CSL\\modern-humanities-research-association.csl\"},\n",
    "    \"13\": {\"name\": \"Modern Language Association 9th edition\",\"path\": r\"G:\\ETL_CSL\\modern-language-association.csl\"},\n",
    "    \"14\": {\"name\": \"Nature\",\"path\": r\"G:\\ETL_CSL\\nature.csl\"},\n",
    "    \"15\": {\"name\": \"Taylor & Francis - Chicago Manual of Style (author-date)\", \"path\": r\"G:\\ETL_CSL\\taylor-and-francis-chicago-author-date.csl\"},\n",
    "    \"16\": {\"name\": \"Vancouver\",\"path\": r\"G:\\ETL_CSL\\vancouver.csl\"},   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PubMed - chunked search\n",
    "# ------------------------------------------------------------------------------\n",
    "def pubmed_search_chunked(query, desired_count=1000, chunk_size=200):\n",
    "    \"\"\"\n",
    "    Searches PubMed for `query`, retrieves up to `desired_count` PMIDs in chunks.\n",
    "    If fewer articles exist, retrieves all available.\n",
    "    \n",
    "    Returns a list of PMIDs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Searching PubMed for: {query}\")\n",
    "    print(f\"[*] Searching PubMed for query: '{query}' ...\")\n",
    "    \n",
    "    # 1) First get total count with a small retmax=0 search\n",
    "    try:\n",
    "        handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=0)\n",
    "        record = Entrez.read(handle)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PubMed query error for '{query}': {e}\", exc_info=True)\n",
    "        print(f\"[!] Error contacting PubMed: {e}\")\n",
    "        return []\n",
    "\n",
    "    total_found = int(record[\"Count\"])\n",
    "    logging.info(f\"Total articles found for '{query}': {total_found}\")\n",
    "    print(f\"[*] PubMed reports {total_found} total articles for this query.\")\n",
    "\n",
    "    # 2) Determine how many we actually want to retrieve\n",
    "    retrieve_count = min(desired_count, total_found)\n",
    "    if retrieve_count == 0:\n",
    "        print(\"[!] No articles to retrieve (retrieve_count=0).\")\n",
    "        return []\n",
    "    \n",
    "    pmid_list = []\n",
    "    start = 0\n",
    "    while start < retrieve_count:\n",
    "        batch = min(chunk_size, retrieve_count - start)\n",
    "        logging.info(f\"Retrieving records {start+1} to {start+batch}...\")\n",
    "        print(f\"    -> Retrieving records {start+1} to {start+batch} ...\")\n",
    "\n",
    "        try:\n",
    "            handle = Entrez.esearch(db=\"pubmed\", term=query, retstart=start, retmax=batch)\n",
    "            record = Entrez.read(handle)\n",
    "            batch_pmids = record.get(\"IdList\", [])\n",
    "            pmid_list.extend(batch_pmids)\n",
    "            start += batch\n",
    "            # Be polite to NCBI servers\n",
    "            time.sleep(0.2)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving chunk {start+1}-{start+batch} from PubMed: {e}\", exc_info=True)\n",
    "            print(f\"[!] Error retrieving chunk {start+1}-{start+batch} from PubMed: {e}\")\n",
    "            break\n",
    "\n",
    "    logging.info(f\"Retrieved {len(pmid_list)} PMIDs (requested up to {retrieve_count}).\")\n",
    "    print(f\"[*] Finished retrieving PMIDs. Got {len(pmid_list)} total.\")\n",
    "    return pmid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) PubMed - fetch metadata\n",
    "# ------------------------------------------------------------------------------\n",
    "def fetch_pubmed_metadata(pmid_list):\n",
    "    \"\"\"\n",
    "    Fetch data for each PMID, including: Title, DOI, Abstract,\n",
    "    [Enhancement] plus Authors, Date, Journal, Volume, Issue, Pages.\n",
    "    Returns a list of dicts suitable for DataFrame creation.\n",
    "    \"\"\"\n",
    "    print(\"[*] Fetching metadata for each PMID...\")\n",
    "    logging.info(f\"Starting metadata fetch for {len(pmid_list)} PMIDs.\")\n",
    "\n",
    "    results = []\n",
    "    for i, pmid in enumerate(pmid_list):\n",
    "        logging.debug(f\"Fetching metadata for PMID {pmid} (index={i}).\")\n",
    "        print(f\"    -> PMID {pmid} ({i+1}/{len(pmid_list)}) ...\")\n",
    "\n",
    "        try:\n",
    "            handle = Entrez.efetch(db='pubmed', id=str(pmid), retmode='xml')\n",
    "            record = Entrez.read(handle)\n",
    "            \n",
    "            article = record['PubmedArticle'][0]['MedlineCitation']['Article']\n",
    "            title = article.get('ArticleTitle', '')\n",
    "            \n",
    "            # Abstract\n",
    "            if 'Abstract' in article and 'AbstractText' in article['Abstract']:\n",
    "                abs_list = article['Abstract']['AbstractText']\n",
    "                abstract = \" \".join(abs_list)\n",
    "            else:\n",
    "                abstract = \"\"\n",
    "            \n",
    "            # Try to get DOI\n",
    "            doi = \"\"\n",
    "            elocation = article.get('ELocationID', [])\n",
    "            for loc in elocation:\n",
    "                if loc.attributes.get('EIdType') == 'doi':\n",
    "                    doi = str(loc)\n",
    "                    break\n",
    "            if not doi:\n",
    "                # Also check ArticleIdList\n",
    "                article_ids = record['PubmedArticle'][0]['PubmedData']['ArticleIdList']\n",
    "                for aid in article_ids:\n",
    "                    if aid.attributes.get('IdType') == 'doi':\n",
    "                        doi = str(aid)\n",
    "                        break\n",
    "            \n",
    "            # [Enhancement] Authors\n",
    "            authors = []\n",
    "            if 'AuthorList' in article:\n",
    "                for author in article['AuthorList']:\n",
    "                    if 'ForeName' in author and 'LastName' in author:\n",
    "                        authors.append({\n",
    "                            \"firstName\": author.get('ForeName', ''),\n",
    "                            \"lastName\": author.get('LastName', '')\n",
    "                        })\n",
    "            \n",
    "            # [Enhancement] Journal, Volume, Issue, Pages, and Date\n",
    "            journal = \"\"\n",
    "            volume = \"\"\n",
    "            issue = \"\"\n",
    "            pages = \"\"\n",
    "            date_str = \"\"\n",
    "\n",
    "            journal_info = article.get('Journal', {})\n",
    "            if journal_info:\n",
    "                journal = journal_info.get('Title', '')\n",
    "                journal_issue_info = journal_info.get('JournalIssue', {})\n",
    "                volume = journal_issue_info.get('Volume', '')\n",
    "                issue = journal_issue_info.get('Issue', '')\n",
    "\n",
    "                # Pages can be in 'Pagination' -> 'MedlinePgn'\n",
    "                pagination = article.get('Pagination', {})\n",
    "                if 'MedlinePgn' in pagination:\n",
    "                    pages = pagination['MedlinePgn']\n",
    "                \n",
    "                # Date can come from 'PubDate' or from 'ArticleDate'\n",
    "                pub_date = journal_issue_info.get('PubDate', {})\n",
    "                # PubDate might have 'Year', 'Month', 'Day'\n",
    "                if 'Year' in pub_date:\n",
    "                    date_str = pub_date['Year']  # e.g. '2020'\n",
    "                elif 'MedlineDate' in pub_date:\n",
    "                    # sometimes it's e.g. '2020 Jan-Feb'\n",
    "                    date_str = pub_date['MedlineDate']\n",
    "            \n",
    "            # Another possible place for date: 'ArticleDate'\n",
    "            if 'ArticleDate' in article and article['ArticleDate']:\n",
    "                # Typically a list of dicts with Year, Month, Day\n",
    "                art_date = article['ArticleDate'][0]\n",
    "                y = art_date.get('Year', '')\n",
    "                m = art_date.get('Month', '')\n",
    "                d = art_date.get('Day', '')\n",
    "                # If we found a year, use it\n",
    "                if y:\n",
    "                    date_str = f\"{y}\"\n",
    "                    if m:\n",
    "                        date_str += f\"-{m}\"\n",
    "                        if d:\n",
    "                            date_str += f\"-{d}\"\n",
    "\n",
    "            results.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Title\": title,\n",
    "                \"DOI\": doi,\n",
    "                \"Abstract\": abstract,\n",
    "                \"Authors\": authors,        # [Enhancement]\n",
    "                \"Date\": date_str,         # [Enhancement]\n",
    "                \"Journal\": journal,       # [Enhancement]\n",
    "                \"Volume\": volume,         # [Enhancement]\n",
    "                \"Issue\": issue,           # [Enhancement]\n",
    "                \"Pages\": pages            # [Enhancement]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching PMID {pmid}: {e}\", exc_info=True)\n",
    "            results.append({\n",
    "                \"PMID\": pmid,\n",
    "                \"Title\": \"\",\n",
    "                \"DOI\": \"\",\n",
    "                \"Abstract\": \"\",\n",
    "                \"Authors\": [],\n",
    "                \"Date\": \"\",\n",
    "                \"Journal\": \"\",\n",
    "                \"Volume\": \"\",\n",
    "                \"Issue\": \"\",\n",
    "                \"Pages\": \"\",\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "\n",
    "    logging.info(f\"Metadata fetch complete. {len(results)} records processed.\")\n",
    "    print(\"[*] Finished fetching metadata.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) PyAlex - OpenAlex search\n",
    "# ------------------------------------------------------------------------------\n",
    "def pyalex_search_works(query, desired_count=50):\n",
    "    \"\"\"\n",
    "    Use PyAlex to search works matching 'query' and retrieve up to 'desired_count' items.\n",
    "    \"\"\"\n",
    "    logging.info(f\"PyAlex: Searching OpenAlex for '{query}' (max={desired_count})\")\n",
    "    print(f\"[*] Searching OpenAlex (PyAlex) for '{query}'...\")\n",
    "\n",
    "    # We'll paginate in chunks of 25\n",
    "    works_iter = Works().search(query).paginate(per_page=25)\n",
    "    results = []\n",
    "    total_fetched = 0\n",
    "\n",
    "    try:\n",
    "        for page in works_iter:\n",
    "            for w in page:\n",
    "                # print(json.dumps(w, indent=2))\n",
    "                # title and abstract\n",
    "                title = w.get(\"display_name\", \"\")\n",
    "                abstract_text = w.get(\"abstract\") or \"\"\n",
    "                \n",
    "                # DOI\n",
    "                doi_full = w.get(\"doi\")\n",
    "                if not isinstance(doi_full, str):\n",
    "                    doi_full = \"\"\n",
    "                doi_full = doi_full.lower()\n",
    "                if doi_full.startswith(\"https://doi.org/\"):\n",
    "                    doi = doi_full.replace(\"https://doi.org/\", \"\")\n",
    "                else:\n",
    "                    doi = doi_full\n",
    "                \n",
    "                # OpenAlex ID\n",
    "                openalex_id = w.get(\"id\", \"\")\n",
    "                \n",
    "                # Authors\n",
    "                authors = []\n",
    "                for auth in w.get(\"authorships\", []):\n",
    "                    author_obj = auth.get(\"author\", {})\n",
    "                    display_name = author_obj.get(\"display_name\", \"\")\n",
    "                    \n",
    "                    if \",\" in display_name:\n",
    "                        parts = display_name.split(\",\", 1)\n",
    "                        last = parts[0].strip()\n",
    "                        first = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                    else:\n",
    "                        sparts = display_name.split()\n",
    "                        if len(sparts) > 1:\n",
    "                            first = sparts[0]\n",
    "                            last = \" \".join(sparts[1:])\n",
    "                        else:\n",
    "                            first = display_name\n",
    "                            last = \"\"\n",
    "                    authors.append({\"firstName\": first, \"lastName\": last})\n",
    "\n",
    "                # Date\n",
    "                date_str = \"\"\n",
    "                if w.get(\"publication_year\"):\n",
    "                    date_str = str(w[\"publication_year\"])\n",
    "                    if w.get(\"publication_date\"):\n",
    "                        date_str = w[\"publication_date\"]  \n",
    "                \n",
    "                # Journal from primary_location->source->display_name\n",
    "                primary_loc = w.get(\"primary_location\") or {}\n",
    "                if not isinstance(primary_loc, dict):\n",
    "                    primary_loc = {}\n",
    "                primary_src = primary_loc.get(\"source\") or {}\n",
    "                if not isinstance(primary_src, dict):\n",
    "                    primary_src = {}\n",
    "                journal = primary_src.get(\"display_name\", \"\")\n",
    "                \n",
    "                # If you want the publisher (e.g. \"Oxford University Press\"):\n",
    "                publisher = primary_src.get(\"host_organization_name\", \"\")\n",
    "                \n",
    "                # If you want the PMID:\n",
    "                pmid_url = w.get(\"ids\", {}).get(\"pmid\", \"\")\n",
    "                pmid_str = pmid_url.split(\"/\")[-1] if pmid_url else \"\"\n",
    "                \n",
    "                biblio = w.get(\"biblio\", {})\n",
    "                volume = biblio.get(\"volume\", \"\")\n",
    "                issue = biblio.get(\"issue\", \"\")\n",
    "                first_p = biblio.get(\"first_page\", \"\")\n",
    "                last_p = biblio.get(\"last_page\", \"\")\n",
    "                pages = \"\"\n",
    "                if first_p and last_p:\n",
    "                    pages = f\"{first_p}-{last_p}\"\n",
    "                elif first_p:\n",
    "                    pages = first_p\n",
    "\n",
    "                    # Collect final metadata\n",
    "                results.append({\n",
    "                    \"PMID\": pmid_str,\n",
    "                    \"Title\": title,\n",
    "                    \"DOI\": doi,\n",
    "                    \"Abstract\": abstract_text,\n",
    "                    \"OpenAlexID\": openalex_id,\n",
    "                    \"Authors\": authors,\n",
    "                    \"Date\": date_str,\n",
    "                    \"Journal\": journal,        # <--- Actual journal name \n",
    "                    \"Publisher\": publisher,    # <--- If you want\n",
    "                    \"Volume\": volume,\n",
    "                    \"Issue\": issue,\n",
    "                    \"Pages\": pages\n",
    "                })\n",
    "\n",
    "                total_fetched += 1\n",
    "                if total_fetched >= desired_count:\n",
    "                    break\n",
    "            if total_fetched >= desired_count:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        logging.error(f\"PyAlex: Error searching OpenAlex for '{query}': {e}\", exc_info=True)\n",
    "        print(f\"[!] Error searching OpenAlex with PyAlex: {e}\")\n",
    "\n",
    "    logging.info(f\"PyAlex: Retrieved {len(results)} results for '{query}'\")\n",
    "    print(f\"[*] PyAlex found {len(results)} works for '{query}'\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) PDF retrieval from Unpaywall\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_pdf_from_unpaywall(doi, email=UNPAYWALL_EMAIL):\n",
    "    \"\"\"\n",
    "    Query Unpaywall for a PDF corresponding to the given DOI.\n",
    "    Returns (pdf_content_bytes, content_type) if found, else (None, None).\n",
    "    \"\"\"\n",
    "    logging.debug(f\"Attempting Unpaywall PDF retrieval for DOI={doi}\")\n",
    "    if not doi:\n",
    "        logging.warning(\"No DOI provided. Skipping PDF retrieval.\")\n",
    "        return None, None\n",
    "    \n",
    "    api_url = f\"https://api.unpaywall.org/v2/{doi}?email={email}\"\n",
    "    try:\n",
    "        resp = requests.get(api_url)\n",
    "        if resp.status_code == 200:\n",
    "            try:\n",
    "                result = resp.json()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"JSON decode failed from Unpaywall for {doi}: {e}\")\n",
    "                return None, None\n",
    "            \n",
    "            if not isinstance(result, dict):\n",
    "                logging.error(f\"Unpaywall returned a non-dict result for {doi}. Skipping.\")\n",
    "                return None, None\n",
    "            \n",
    "            best_pdf = result.get(\"best_oa_location\") or {}\n",
    "            pdf_url = best_pdf.get(\"url_for_pdf\")\n",
    "            \n",
    "            if pdf_url:\n",
    "                logging.debug(f\"Found best_oa_location PDF: {pdf_url}\")\n",
    "                try:\n",
    "                    pdf_resp = requests.get(pdf_url, timeout=20)\n",
    "                    if (pdf_resp.status_code == 200 \n",
    "                            and \"pdf\" in pdf_resp.headers.get(\"Content-Type\", \"\").lower()):\n",
    "                        return pdf_resp.content, pdf_resp.headers[\"Content-Type\"]\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to fetch best_oa_location PDF for {doi}: {e}\")                  \n",
    "                    \n",
    "         # fallback: check all oa_locations\n",
    "            oa_locs = result.get(\"oa_locations\", [])\n",
    "            if isinstance(oa_locs, list):\n",
    "                for loc in oa_locs:\n",
    "                    if not isinstance(loc, dict):\n",
    "                        continue\n",
    "                    pdf_url = loc.get(\"url_for_pdf\")\n",
    "                    if pdf_url:\n",
    "                        logging.debug(f\"[Unpaywall fallback] Trying {pdf_url}\")\n",
    "                        try:\n",
    "                            pdf_resp = requests.get(pdf_url, timeout=20)\n",
    "                            if (pdf_resp.status_code == 200 \n",
    "                                    and \"pdf\" in pdf_resp.headers.get(\"Content-Type\", \"\").lower()):\n",
    "                                return pdf_resp.content, pdf_resp.headers[\"Content-Type\"]\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Failed fallback PDF for {doi}: {e}\")\n",
    "                            \n",
    "            logging.info(f\"No valid PDF link found for DOI={doi}.\")\n",
    "        else:\n",
    "            logging.error(f\"Unpaywall error. status={resp.status_code} for DOI={doi}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception retrieving PDF from Unpaywall for {doi}: {e}\", exc_info=True)\n",
    "    \n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Azure PDF Extraction\n",
    "# ------------------------------------------------------------------------------\n",
    "class AzurePDFExtractor:\n",
    "    \"\"\"\n",
    "    Wraps Azure DocumentIntelligenceClient to extract text and tables via the 'prebuilt-layout' model.\n",
    "    \"\"\"\n",
    "    def __init__(self, endpoint, key):\n",
    "        self.client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    def extract_text_and_tables(self, pdf_path: str):\n",
    "        logging.debug(f\"Starting Azure extraction for: {pdf_path}\")\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            poller = self.client.begin_analyze_document(\"prebuilt-layout\", body=f)\n",
    "        result: AnalyzeResult = poller.result()\n",
    "\n",
    "        all_pages_text = []\n",
    "        for page in result.pages:\n",
    "            page_lines = []\n",
    "            if page.lines:\n",
    "                for line in page.lines:\n",
    "                    page_lines.append(line.content)\n",
    "            page_text = \"\\n\".join(page_lines)\n",
    "            all_pages_text.append(page_text)\n",
    "        full_text = \"\\n\\n\".join(all_pages_text)\n",
    "\n",
    "        table_dataframes = []\n",
    "        if result.tables:\n",
    "            for table in result.tables:\n",
    "                matrix = [\n",
    "                    [\"\" for _ in range(table.column_count)]\n",
    "                    for _ in range(table.row_count)\n",
    "                ]\n",
    "                for cell in table.cells:\n",
    "                    matrix[cell.row_index][cell.column_index] = cell.content or \"\"\n",
    "                df_table = pd.DataFrame(matrix)\n",
    "                table_dataframes.append(df_table)\n",
    "\n",
    "        logging.debug(f\"Azure extraction complete for: {pdf_path}\")\n",
    "        return full_text, table_dataframes\n",
    "# Why is AzurePDFExtractor declared as a class?\n",
    "# The short answer: Object-Oriented organization. It’s a small “wrapper class” that holds:\n",
    "## 1) A client (DocumentIntelligenceClient) as a member (in __init__), and\n",
    "## 2)  A method (extract_text_and_tables) that uses that client.\n",
    "# This means whenever you instantiate an AzurePDFExtractor, you automatically get:\n",
    "## 1) The Azure Document Intelligence client, already set up with endpoint and key.\n",
    "## 2) A reusable method (.extract_text_and_tables(...)) to parse PDFs.\n",
    "# In functional terms, it’s just a convenient way to bundle (1) the Azure credentials and (2) the parsing logic, rather than having a bunch of separate functions with extra parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Deduplicate & Merge\n",
    "# ------------------------------------------------------------------------------\n",
    "def deduplicate_by_doi_title(df):\n",
    "    \"\"\"\n",
    "    Remove duplicates within 'df' by checking:\n",
    "      - 'DOI' (case-insensitive)\n",
    "      - fallback: Title if 'DOI' is empty\n",
    "    Return a deduplicated dataframe.\n",
    "    \"\"\"\n",
    "    # Create normalized columns\n",
    "    df[\"DOI_clean\"] = df[\"DOI\"].astype(str).str.lower().str.strip()\n",
    "    df[\"Title_clean\"] = df[\"Title\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Sort to keep first occurrence\n",
    "    df.sort_values(by=[\"DOI_clean\",\"Title_clean\"], inplace=True)\n",
    "\n",
    "    # Drop duplicates by doi\n",
    "    df.drop_duplicates(subset=[\"DOI_clean\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    # For any with empty doi, also check Title\n",
    "    df.drop_duplicates(subset=[\"Title_clean\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    df.drop(columns=[\"DOI_clean\",\"Title_clean\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Utility: Add a PDF to Zotero, generate item, attach PDF, retrieve metadata\n",
    "# ------------------------------------------------------------------------------\n",
    "def add_pdf_to_zotero(pdf_path, metadata, collection_keys=None):\n",
    "    \"\"\"\n",
    "    Adds the PDF to Zotero with the provided metadata and returns the new item key.\n",
    "    This function:\n",
    "      1) Creates a 'journalArticle' item with fields (title, DOI, authors, date, etc.).\n",
    "      2) Attaches the PDF to that item.\n",
    "      3) Returns the Zotero item key if successful, else None.\n",
    "      \n",
    "      Parameters:\n",
    "        pdf_path (str): Path to the downloaded PDF.\n",
    "        metadata (dict): Dictionary containing metadata (e.g., Title, DOI, Authors, etc.).\n",
    "        collection_keys (list, optional): List of Zotero collection keys where the item should be placed.\n",
    "    \n",
    "    Returns:\n",
    "      item_key (str): The Zotero item key if successful, else None.\n",
    "\n",
    "    'metadata' is expected to be a dict that may have the following keys:\n",
    "      - \"Title\" (string)\n",
    "      - \"DOI\" (string)\n",
    "      - \"Authors\" (list of dict, each with e.g. {\"firstName\": \"...\", \"lastName\": \"...\"})\n",
    "      - \"Date\" (string, e.g. \"2019\" or \"2019-05-10\")\n",
    "      - \"Journal\" (string, if you want to store publication title)\n",
    "      - \"Volume\", \"Pages\", \"Issue\" (optional strings)\n",
    "    \n",
    "    Example of metadata:\n",
    "    {\n",
    "      \"Title\": \"A SWAT-based optimization tool ...\",\n",
    "      \"DOI\": \"10.1016/j.scitotenv.2019.07.175\",\n",
    "      \"Authors\": [\n",
    "        {\"firstName\": \"Y.\", \"lastName\": \"Liu\"},\n",
    "        {\"firstName\": \"T.\", \"lastName\": \"Guo\"},\n",
    "        # ...\n",
    "      ],\n",
    "      \"Date\": \"2019\",\n",
    "      \"Journal\": \"Science of The Total Environment\",\n",
    "      \"Volume\": \"691\",\n",
    "      \"Pages\": \"685-696\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Use the default collection key if none are provided\n",
    "    if collection_keys is None:\n",
    "        collection_keys = [DEFAULT_ZOTERO_COLLECTION]\n",
    "\n",
    "    # 2) Start the item_data with minimal required fields\n",
    "    item_data = {\n",
    "        \"itemType\": \"journalArticle\",\n",
    "        \"title\": metadata.get(\"Title\", \"\"),   # from user metadata\n",
    "        \"DOI\": metadata.get(\"DOI\", \"\"),       # from user metadata\n",
    "        \"collections\": collection_keys\n",
    "    }\n",
    "\n",
    "    # 3) Build the list of creators (authors, editors, etc.) if provided\n",
    "    authors_list = metadata.get(\"Authors\", [])\n",
    "    if authors_list and isinstance(authors_list, list):\n",
    "        # Prepare the creators field for Zotero\n",
    "        zotero_creators = []\n",
    "        for author in authors_list:\n",
    "            # Each 'author' is a dict with e.g. {\"firstName\": \"Y.\", \"lastName\": \"Liu\"}\n",
    "            zotero_creators.append({\n",
    "                \"creatorType\": \"author\",\n",
    "                \"firstName\": author.get(\"firstName\", \"\"),\n",
    "                \"lastName\": author.get(\"lastName\", \"\")\n",
    "            })\n",
    "        # Now attach this to item_data\n",
    "        item_data[\"creators\"] = zotero_creators\n",
    "\n",
    "    # 4) Date, Journal, Volume, Pages, etc. are optional but help build robust citations\n",
    "    #    Only add them if they exist in metadata\n",
    "    if \"Date\" in metadata:\n",
    "        item_data[\"date\"] = metadata[\"Date\"]\n",
    "    if \"Journal\" in metadata:\n",
    "        item_data[\"publicationTitle\"] = metadata[\"Journal\"]\n",
    "    if \"Volume\" in metadata:\n",
    "        item_data[\"volume\"] = metadata[\"Volume\"]\n",
    "    if \"Issue\" in metadata:\n",
    "        item_data[\"issue\"] = metadata[\"Issue\"]\n",
    "    if \"Pages\" in metadata:\n",
    "        item_data[\"pages\"] = metadata[\"Pages\"]\n",
    "    if \"Abstract\" in metadata:\n",
    "        item_data[\"abstractNote\"] = metadata[\"Abstract\"]\n",
    "    if \"Publisher\" in metadata:\n",
    "        item_data[\"publisher\"] = metadata[\"Publisher\"]\n",
    "    if \"PMID\" in metadata and metadata[\"PMID\"]:\n",
    "        pmid_str = f\"PMID: {metadata['PMID']}\"\n",
    "        # If there's already \"extra\", append; else create\n",
    "        if \"extra\" in item_data and item_data[\"extra\"]:\n",
    "            item_data[\"extra\"] += f\"\\n{pmid_str}\"\n",
    "        else:\n",
    "            item_data[\"extra\"] = pmid_str         \n",
    "        \n",
    "    try:\n",
    "        # 5) Create the item in Zotero\n",
    "        created_items = zot.create_items([item_data])\n",
    "        # Zotero's response is usually a dict with a \"successful\" key.\n",
    "        if isinstance(created_items, dict):\n",
    "            successful_items = created_items.get(\"successful\", {})\n",
    "            if not successful_items:\n",
    "                logging.error(\"Zotero create_items did not return any successful items.\")\n",
    "                return None\n",
    "        # 6) Extract the item key from the response\n",
    "            first_key = list(successful_items.keys())[0]\n",
    "            item_data_resp = successful_items[first_key]\n",
    "            item_key = item_data_resp.get(\"data\", {}).get(\"key\")\n",
    "        elif isinstance(created_items, list) and created_items:\n",
    "            item_key = created_items[0].get(\"data\", {}).get(\"key\")\n",
    "        else:\n",
    "            logging.error(\"Zotero create_items did not return a valid list.\")\n",
    "            return None\n",
    "        if not item_key:\n",
    "            logging.warning(\"No item key retrieved from Zotero creation.\")\n",
    "            return None\n",
    "        # 7) Attach the PDF to the newly created item\n",
    "        try:\n",
    "            zot.attachment_simple(\n",
    "                files=[pdf_path],\n",
    "                parentid=item_key,\n",
    "            )\n",
    "            logging.info(f\"PDF attached to Zotero item {item_key}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error attaching PDF to Zotero item {item_key}: {e}\", exc_info=True)\n",
    "        return item_key\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error adding PDF to Zotero: {e}\", exc_info=True)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) retrieve the metadata from Zotero via Pyzotero and then use a CSL (Citation Style Language) processor to format the citations in the desired style (e.g., APA)\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_citation_csl_json(item_key, style):\n",
    "    \"\"\"\n",
    "    Retrieves item metadata from Zotero, converts it to CSL JSON,\n",
    "    then uses citeproc-py to generate both an in-text and a full citation.\n",
    "    \n",
    "    Returns (in_text_citation, full_citation).\n",
    "    \"\"\"\n",
    "# 1) Fetch the item from Zotero\n",
    "    try:\n",
    "        item = zot.item(item_key)\n",
    "        data = item.get(\"data\", {})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving Zotero item {item_key}: {e}\")\n",
    "        return (None, None)\n",
    "\n",
    "    # Convert Zotero data to something close to CSL JSON. For a real approach,\n",
    "    # you'd map Zotero fields (creators -> author, date -> issued, etc.) more fully.\n",
    "    # This minimal example just sets a few keys:\n",
    "    # Map Zotero data to minimal CSL JSON format.\n",
    "    csl_item = {\n",
    "        \"id\": item_key,\n",
    "        \"type\": \"article-journal\",\n",
    "        \"title\": data.get(\"title\", \"\"),\n",
    "        \"DOI\": data.get(\"DOI\", \"\"),\n",
    "        # For authors, if in \"creators\", map them to CSL \"author\"\n",
    "        \"author\": []\n",
    "    }\n",
    "    \n",
    "    for creator in data.get(\"creators\", []):\n",
    "        if creator.get(\"creatorType\") in (\"author\", \"editor\", \"contributor\"):\n",
    "            csl_item[\"author\"].append({\n",
    "                \"family\": creator.get(\"lastName\", \"Unknown\"),\n",
    "                \"given\": creator.get(\"firstName\", \"\")\n",
    "            })\n",
    "            \n",
    "    # Process the date.\n",
    "    date_str = data.get(\"date\", \"\")\n",
    "    if date_str:\n",
    "        # Attempt to split on non-digits to create a date-parts list.\n",
    "        # For example, \"2023\" becomes [[2023]] and \"2023-05-10\" becomes [[2023, 5, 10]]\n",
    "        try:\n",
    "            parts = [int(x) for x in re.split(r'\\D+', date_str) if x.isdigit()]\n",
    "            if parts:\n",
    "                csl_item[\"issued\"] = {\"date-parts\": [parts]}\n",
    "            else:\n",
    "                csl_item[\"issued\"] = {\"raw\": date_str}\n",
    "        except Exception:\n",
    "            csl_item[\"issued\"] = {\"raw\": date_str}\n",
    "    \n",
    "    # Map additional bibliographic fields.\n",
    "    if data.get(\"publicationTitle\"):\n",
    "        csl_item[\"container-title\"] = data.get(\"publicationTitle\")\n",
    "    if data.get(\"volume\"):\n",
    "        csl_item[\"volume\"] = data.get(\"volume\")\n",
    "    if data.get(\"issue\"):\n",
    "        csl_item[\"issue\"] = data.get(\"issue\")\n",
    "    if data.get(\"pages\"):\n",
    "        csl_item[\"page\"] = data.get(\"pages\")\n",
    "        \n",
    "        # [Enhancement #1] Abstract -> csl_item[\"abstract\"]\n",
    "    abstract_str = data.get(\"abstractNote\", \"\")\n",
    "    if abstract_str:\n",
    "        csl_item[\"abstract\"] = abstract_str\n",
    "    \n",
    "    # [Enhancement #2] Publisher (uncommon for journalArticle, but possible)\n",
    "    publisher = data.get(\"publisher\", \"\")\n",
    "    if publisher:\n",
    "        csl_item[\"publisher\"] = publisher\n",
    "\n",
    "    # [Enhancement #3] URL -> csl_item[\"URL\"]\n",
    "    url_str = data.get(\"url\", \"\")\n",
    "    if url_str:\n",
    "        csl_item[\"URL\"] = url_str\n",
    "    \n",
    "    # [Enhancement #4] Extract PMID from extra field\n",
    "    pmid_val = None\n",
    "    extra_str = data.get(\"extra\", \"\")\n",
    "    if \"PMID:\" in extra_str:\n",
    "        for line in extra_str.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.upper().startswith(\"PMID:\"):\n",
    "                pmid_val = line.split(\":\", 1)[1].strip()\n",
    "                break\n",
    "    if pmid_val:\n",
    "        # Not a standard CSL field, but storing it doesn't hurt\n",
    "        csl_item[\"PMID\"] = pmid_val\n",
    "    \n",
    "    # Build the CiteProc JSON\n",
    "\n",
    "    csl_source = CiteProcJSON([csl_item])\n",
    "    \n",
    "# 2) Load a citation style (apa, etc.)\n",
    "# If 'style' is a path on disk, treat it as a local .csl file\n",
    "    try:\n",
    "        if os.path.isfile(style):\n",
    "            csl_style = CitationStylesStyle(style, locale=\"en-US\", validate=False)\n",
    "        else:\n",
    "            # e.g. if user typed \"apa\" or \"ieee\"\n",
    "            raise ValueError(f\"Style file {style} not found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading CSL style from {style}: {e}\")\n",
    "        return(None, None)\n",
    "            \n",
    "            \n",
    "# 3) Create a bibliography object\n",
    "    bibliography = CitationStylesBibliography(csl_style, csl_source, formatter.plain)\n",
    "# For an in-text citation, we create a Citation object referencing the item ID\n",
    "    citation = Citation([CitationItem(item_key)])\n",
    "# Register this citation\n",
    "    bibliography.register(citation)   \n",
    "# 4) The first output from bibliography is the in-text citation\n",
    "# Use the cite() method with a warning callback.\n",
    "    def warn(citation_item):\n",
    "        logging.warning(f\"Reference with key '{citation_item.key}' not found.\")\n",
    "    in_text_citation = bibliography.cite(citation, warn)\n",
    "    bib_entries = bibliography.bibliography()\n",
    "    full_citation = str(bib_entries[0]) if bib_entries else \"\"\n",
    "    return (in_text_citation, full_citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_additional_fields(feather_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Loads the DataFrame from 'feather_path', then for each row, combines\n",
    "    FullText + TablesJson, chunks if needed, and extracts the following fields:\n",
    "      StudyType, Scenario, Agronomic or Management Scenarios,\n",
    "      Management Practices and Operational Details, Economic and Agronomic Benefits,\n",
    "      Regional and Climatic Variability, Tillage Practices, Soil Type/Characteristics,\n",
    "      Cover Crop Type, Metrics (array), QA_Note\n",
    "\n",
    "    The function also stores the entire extracted record as JSON in a column called 'AllExtracted'.\n",
    "\n",
    "    If 'output_path' is given, saves the enriched DataFrame back to Feather.\n",
    "    Otherwise, just returns the in-memory DataFrame.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading DataFrame from {feather_path} ...\")\n",
    "    print(f\"Loading DataFrame from {feather_path} ...\")\n",
    "    try:\n",
    "        df = pd.read_feather(feather_path)\n",
    "        print(\"DataFrame loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading feather {feather_path}: {e}\", exc_info=True)\n",
    "        print(f\"Error reading feather {feather_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2) Helper function: Combine FullText and TablesJson into one string\n",
    "    def combine_text_and_tables(full_text, tables_json_str):\n",
    "        try:\n",
    "            table_text = \"\"\n",
    "            if isinstance(tables_json_str, str) and tables_json_str.strip():\n",
    "                try:\n",
    "                    tables_data = json.loads(tables_json_str)\n",
    "                    for i, table_rows in enumerate(tables_data):\n",
    "                        table_text += f\"Table #{i}:\\n\"\n",
    "                        if isinstance(table_rows, list):\n",
    "                            for row_dict in table_rows:\n",
    "                                try:\n",
    "                                    row_str = \", \".join(str(v) for v in row_dict.values())\n",
    "                                    table_text += row_str + \"\\n\"\n",
    "                                except Exception as e:\n",
    "                                    logging.error(f\"Error processing row in table {i}: {e}\", exc_info=True)\n",
    "                        table_text += \"\\n\"\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error parsing TablesJson: {e}\", exc_info=True)\n",
    "                    table_text += \"[Could not parse TablesJson]\\n\"\n",
    "            combined = (full_text or \"\") + \"\\n\\n[TABLES]\\n\" + table_text\n",
    "            return combined.strip()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error combining text and tables: {e}\", exc_info=True)\n",
    "            return full_text\n",
    "\n",
    "    # 3) Helper function: Chunk the text (using a simple substring approach)\n",
    "    def chunk_text_for_extraction(full_str, max_tokens=20000, overlap=1000, model_name=MODEL_NAME):\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(model_name)\n",
    "            tokens = enc.encode(full_str)\n",
    "            chunks = []\n",
    "            start = 0\n",
    "            while start < len(tokens):\n",
    "                end = start + max_tokens\n",
    "                chunk_tokens = tokens[start:end]\n",
    "                chunk_part = enc.decode(chunk_tokens)\n",
    "                chunks.append(chunk_part)\n",
    "                # Move start forward but allow overlap\n",
    "                start = max(0, end - overlap)\n",
    "            return chunks\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error chunking text: {e}\", exc_info=True)\n",
    "            return [full_str]\n",
    "\n",
    "    # 4) Prompt template for extraction\n",
    "    extraction_template = \"\"\"\n",
    "You are an expert data-extraction agent focusing on additional metadata for cover crops and greenhouse gas data.\n",
    "\n",
    "Below is the article's known metadata (some fields may be empty):\n",
    "{metadata_json}\n",
    "\n",
    "Below is a chunk of text from the article's FullText + Tables:\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\n",
    "Your primary goal: **extract** or **update** the following fields in a JSON object:\n",
    "\n",
    "1) \"Title\"\n",
    "2) \"StudyType\"\n",
    "3) \"Scenario\"\n",
    "4) \"Agronomic or Management Scenarios\"\n",
    "5) \"Management Practices and Operational Details\"\n",
    "6) \"Economic and Agronomic Benefits\"\n",
    "7) \"Regional and Climatic Variability\"\n",
    "8) \"Tillage Practices\"\n",
    "9) \"Soil Type/Characteristics\"\n",
    "10) \"Cover Crop Type\"\n",
    "11) \"Metrics\" (array of objects) — numeric data for GWP, SOC, yield, N₂O, etc. \n",
    "    - Each object must have exactly: {{ \"Metric\": \"...\", \"Value\": \"...\", \"Unit\": \"...\" }}\n",
    "12) \"QA_Note\" — if no numeric data are found, set this to \"No numeric data found.\"\n",
    "\n",
    "**No other keys** should appear in your output JSON.  \n",
    "If the chunk does not contain new info, just **return the same JSON** object unchanged.\n",
    "\n",
    "## Detailed Instructions\n",
    "1. Read the existing JSON object (some fields may already be populated).\n",
    "2. Read the text chunk above. If new data is found that updates or refines these fields, overwrite or fill them in.\n",
    "3. If numeric data is found (e.g., N2O = 1.2 kg/ha), place it under \"Metrics\" as an object with \"Metric\", \"Value\", and \"Unit\".\n",
    "4. If **no** numeric data is found, set `\"Metrics\": []` and `\"QA_Note\": \"No numeric data found.\"`\n",
    "5. Return a **single JSON object** with exactly these 12 fields (do not add or rename any fields).\n",
    "6. If you see references to machine learning or special conditions (like “random forest”), mention them briefly in \"QA_Note\".\n",
    "\n",
    "## Example Output\n",
    "\n",
    "json\n",
    "{{\n",
    "  \"Title\": \"Example Cover Crop Study\",\n",
    "  \"StudyType\": \"On-farm trial\",\n",
    "  \"Scenario\": \"Reduced tillage with barley\",\n",
    "  \"Agronomic or Management Scenarios\": \"Early planting vs late planting\",\n",
    "  \"Management Practices and Operational Details\": \"Seeds planted Oct 1, terminated May 15, 100 kg N fertilizer\",\n",
    "  \"Economic and Agronomic Benefits\": \"Yield increased by 5%, moderate cost savings\",\n",
    "  \"Regional and Climatic Variability\": \"Mid-Atlantic region, 1200 mm rainfall\",\n",
    "  \"Tillage Practices\": \"No-till\",\n",
    "  \"Soil Type/Characteristics\": \"Loam, pH 6.2\",\n",
    "  \"Cover Crop Type\": \"Barley\",\n",
    "  \"Metrics\": [\n",
    "    {{\n",
    "      \"Metric\": \"N2O\",\n",
    "      \"Value\": \"3.1\",\n",
    "      \"Unit\": \"kg N/ha\"\n",
    "    }}\n",
    "  ],\n",
    "  \"QA_Note\": \"Mentioned random-forest approach for yield prediction.\"\n",
    "}}\n",
    "Return only the final updated JSON (no extra commentary).\n",
    " \n",
    "\"\"\"\n",
    "    # 5) Helper function: Merge new extracted data into the base record\n",
    "    def unify_fields(existing_data, new_data):\n",
    "        try:\n",
    "            fields = [\n",
    "                \"StudyType\", \"Scenario\", \"Agronomic or Management Scenarios\",\n",
    "                \"Management Practices and Operational Details\", \"Economic and Agronomic Benefits\",\n",
    "                \"Regional and Climatic Variability\", \"Tillage Practices\",\n",
    "                \"Soil Type/Characteristics\", \"Cover Crop Type\", \"Metrics\", \"QA_Note\"\n",
    "            ]\n",
    "            for f in fields:\n",
    "                try:\n",
    "                    if f in new_data and new_data[f]:\n",
    "                        if f == \"Metrics\":\n",
    "                            if not existing_data.get(f):\n",
    "                                existing_data[f] = []\n",
    "                            for m in new_data[f]:\n",
    "                                if m not in existing_data[f]:\n",
    "                                    existing_data[f].append(m)\n",
    "                        else:\n",
    "                            existing_data[f] = new_data[f]\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error updating field {f}: {e}\", exc_info=True)\n",
    "            return existing_data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in unify_fields: {e}\", exc_info=True)\n",
    "            return existing_data\n",
    "\n",
    "    # 6) Add new columns to the DataFrame if they don't exist\n",
    "    new_cols = [\n",
    "        \"StudyType\", \"Scenario\", \"Agronomic or Management Scenarios\",\n",
    "        \"Management Practices and Operational Details\", \"Economic and Agronomic Benefits\",\n",
    "        \"Regional and Climatic Variability\", \"Tillage Practices\",\n",
    "        \"Soil Type/Characteristics\", \"Cover Crop Type\",\n",
    "        \"Metrics\", \"QA_Note\"\n",
    "    ]\n",
    "    for c in new_cols:\n",
    "        try:\n",
    "            if c not in df.columns:\n",
    "                df[c] = None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error adding column {c} to DataFrame: {e}\", exc_info=True)\n",
    "    try:\n",
    "        if \"AllExtracted\" not in df.columns:\n",
    "            df[\"AllExtracted\"] = None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error adding 'AllExtracted' column: {e}\", exc_info=True)\n",
    "\n",
    "    # 7) Main loop: Process each row of the DataFrame\n",
    "    for i in range(len(df)):\n",
    "        print(f\"Processing row {i} ...\")\n",
    "        try:\n",
    "            base_record = {\n",
    "                \"Title\": df.at[i, \"Title\"] or \"\",\n",
    "                \"StudyType\": df.at[i, \"StudyType\"] or \"\",\n",
    "                \"Scenario\": df.at[i, \"Scenario\"] or \"\",\n",
    "                \"Agronomic or Management Scenarios\": df.at[i, \"Agronomic or Management Scenarios\"] or \"\",\n",
    "                \"Management Practices and Operational Details\": df.at[i, \"Management Practices and Operational Details\"] or \"\",\n",
    "                \"Economic and Agronomic Benefits\": df.at[i, \"Economic and Agronomic Benefits\"] or \"\",\n",
    "                \"Regional and Climatic Variability\": df.at[i, \"Regional and Climatic Variability\"] or \"\",\n",
    "                \"Tillage Practices\": df.at[i, \"Tillage Practices\"] or \"\",\n",
    "                \"Soil Type/Characteristics\": df.at[i, \"Soil Type/Characteristics\"] or \"\",\n",
    "                \"Cover Crop Type\": df.at[i, \"Cover Crop Type\"] or \"\",\n",
    "                \"Metrics\": df.at[i, \"Metrics\"] if (\"Metrics\" in df.columns and df.at[i, \"Metrics\"]) else [],\n",
    "                \"QA_Note\": df.at[i, \"QA_Note\"] if (\"QA_Note\" in df.columns and df.at[i, \"QA_Note\"]) else \"\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error building base record for row {i}: {e}\", exc_info=True)\n",
    "            print(f\"Skipping row {i} due to error building base record.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            full_text = df.at[i, \"FullText\"] or \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving FullText for row {i}: {e}\", exc_info=True)\n",
    "            full_text = \"\"\n",
    "\n",
    "        try:\n",
    "            tables_json_str = df.at[i, \"TablesJson\"] or \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving TablesJson for row {i}: {e}\", exc_info=True)\n",
    "            tables_json_str = \"\"\n",
    "\n",
    "        try:\n",
    "            combined_str = combine_text_and_tables(full_text, tables_json_str)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error combining text and tables for row {i}: {e}\", exc_info=True)\n",
    "            combined_str = full_text\n",
    "\n",
    "        if not combined_str.strip() or combined_str == \"ANALYSIS_ERROR\":\n",
    "            logging.debug(f\"Row {i}: No meaningful text/tables to parse.\")\n",
    "            print(f\"Row {i}: Skipping because no meaningful text/tables found.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            chunks = chunk_text_for_extraction(combined_str, max_tokens=20000, overlap=1000)\n",
    "            print(f\"Row {i}: Text chunked into {len(chunks)} chunks.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error chunking combined text for row {i}: {e}\", exc_info=True)\n",
    "            chunks = [combined_str]\n",
    "\n",
    "        # Process each text chunk\n",
    "        for c_i, chunk_txt in enumerate(chunks):\n",
    "            print(f\"Row {i}, Chunk {c_i}: Sending prompt to OpenAI.\")\n",
    "            try:\n",
    "                # Escape curly braces in the JSON string\n",
    "                metadata_json_str = json.dumps(base_record, indent=2).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "                prompt_str = extraction_template.format(\n",
    "                    metadata_json=metadata_json_str,\n",
    "                    chunk_text=chunk_txt\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error formatting prompt for row {i}, chunk {c_i}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                resp = openai.ChatCompletion.create(\n",
    "                    model=\"o3-mini\",  # or your chosen model\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt_str}],\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error calling OpenAI for row {i}, chunk {c_i}: {e}\", exc_info=True)\n",
    "                print(f\"Row {i}, Chunk {c_i}: OpenAI API call failed: {e}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                content = resp.choices[0].message.content.strip()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error accessing response content for row {i}, chunk {c_i}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                new_data = json.loads(content)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Could not parse JSON for row {i}, chunk {c_i}: {e}\", exc_info=True)\n",
    "                print(f\"Row {i}, Chunk {c_i}: JSON parsing failed.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                base_record = unify_fields(base_record, new_data)\n",
    "                print(f\"Row {i}, Chunk {c_i}: Successfully merged new data.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error merging new data for row {i}, chunk {c_i}: {e}\", exc_info=True)\n",
    "\n",
    "        # End chunk loop – update the DataFrame with the final base_record for this row\n",
    "        for c in new_cols:\n",
    "            try:\n",
    "                if c == \"Metrics\" and isinstance(base_record.get(c), list):\n",
    "                    df.at[i, c] = base_record[c]\n",
    "                else:\n",
    "                    df.at[i, c] = base_record.get(c, \"\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error storing field {c} for row {i}: {e}\", exc_info=True)\n",
    "\n",
    "        try:\n",
    "            df.at[i, \"AllExtracted\"] = json.dumps(base_record)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error storing AllExtracted for row {i}: {e}\", exc_info=True)\n",
    "\n",
    "        logging.debug(f\"Row {i} extraction done: {base_record}\")\n",
    "        print(f\"Row {i}: Extraction complete.\")\n",
    "\n",
    "    # End main loop\n",
    "\n",
    "    # 8) Save the updated DataFrame if output_path is provided\n",
    "    if output_path:\n",
    "        try:\n",
    "            df.to_feather(output_path)\n",
    "            logging.info(f\"Enriched DataFrame saved to {output_path}\")\n",
    "            print(f\"Enriched DataFrame saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving enriched DataFrame: {e}\", exc_info=True)\n",
    "            print(f\"Error saving enriched DataFrame: {e}\")\n",
    "    else:\n",
    "        logging.info(\"Extraction complete. DataFrame not saved because output_path is None.\")\n",
    "        print(\"Extraction complete. DataFrame not saved.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting ETL + RAG Pipeline ===\n",
      "[Setp 1/11] Enter multiple queries, one per line. Press ENTER on a blank line to finish.\n",
      "[Step 2/10] Choose your citation style:\n",
      "   1) American Medical Association 11th edition\n",
      "   2) American Psychological Association (APA) 6th Edition \n",
      "   3) American Psychological Association (APA) 7th Edition \n",
      "   4) American Psychological Association (APA) 7th Edition (annotated bibliography)\n",
      "   5) American Political Science Association\n",
      "   6) Chicago Manual of Style 17th Edition (author-date)\n",
      "   7) Chicago Manual of Style 17th Edition (full note)\n",
      "   8) Chicago Manual of Style 17th edition (note)\n",
      "   9) Cite Them Right 12th edition - Harvard\n",
      "   10) Elsevier - Harvard (with titles)\n",
      "   11) IEEE\n",
      "   12) Modern Humanities Research Association 4th edition (notes with bibliography)\n",
      "   13) Modern Language Association 9th edition\n",
      "   14) Nature\n",
      "   15) Taylor & Francis - Chicago Manual of Style (author-date)\n",
      "   16) Vancouver\n",
      "[*] You selected style: American Psychological Association (APA) 6th Edition \n",
      "[Step 3/11] Choose your data source(s):\n",
      "   1) PubMed only\n",
      "   2) OpenAlex (PyAlex) only\n",
      "   3) Both PubMed + PyAlex\n",
      "[*] You asked for up to 14 articles per source.\n",
      "[*] Will retrieve up to 14 articles per query for each source.\n",
      "[Step 4/11]\n",
      "=== PROCESSING QUERY: '\"cover crops\" \"carbon sequestration\"' ===\n",
      "[*] Searching PubMed for query: '\"cover crops\" \"carbon sequestration\"' ...\n",
      "[*] PubMed reports 40 total articles for this query.\n",
      "    -> Retrieving records 1 to 14 ...\n",
      "[*] Finished retrieving PMIDs. Got 14 total.\n",
      "[*] Fetching metadata for each PMID...\n",
      "    -> PMID 39639014 (1/14) ...\n",
      "    -> PMID 39486296 (2/14) ...\n",
      "    -> PMID 39065486 (3/14) ...\n",
      "    -> PMID 38894582 (4/14) ...\n",
      "    -> PMID 38731357 (5/14) ...\n",
      "    -> PMID 38717995 (6/14) ...\n",
      "    -> PMID 38611549 (7/14) ...\n",
      "    -> PMID 38498443 (8/14) ...\n",
      "    -> PMID 38423338 (9/14) ...\n",
      "    -> PMID 38363404 (10/14) ...\n",
      "    -> PMID 38354806 (11/14) ...\n",
      "    -> PMID 38320700 (12/14) ...\n",
      "    -> PMID 38273485 (13/14) ...\n",
      "    -> PMID 38259938 (14/14) ...\n",
      "[*] Finished fetching metadata.\n",
      "     -> PubMed Returned 14 articles for '\"cover crops\" \"carbon sequestration\".\n",
      "[*] Searching OpenAlex (PyAlex) for '\"cover crops\" \"carbon sequestration\"'...\n",
      "[*] PyAlex found 14 works for '\"cover crops\" \"carbon sequestration\"'\n",
      "    -> OpenAlex returned 14 articles for '\"cover crops\" \"carbon sequestration\"'.\n",
      "\n",
      "=== Summary of Article Hits per Query ===\n",
      " - Query '\"cover crops\" \"carbon sequestration\"': PubMed=14, OpenAlex=14, combined=28\n",
      "[Step 5/11]\n",
      "Merging all partial results and deduplicating...\n",
      "[*] After deduplication, total unique articles: 28\n",
      "[Step 7/11] Attempting to download PDFs via Unpaywall...\n",
      " -> Downloading PDF for row 0, Title='Cover crops and carbon sequest' ...\n",
      " -> Downloading PDF for row 1, Title='Carbon sequestration in agricu' ...\n",
      " -> Downloading PDF for row 2, Title='Synergistic water quality and ' ...\n",
      " -> Downloading PDF for row 3, Title='Real cover crops contribution ' ...\n",
      " -> Downloading PDF for row 4, Title='Cover crop cultivation strateg' ...\n",
      " -> Downloading PDF for row 5, Title='Assessing the potential of nat' ...\n",
      " -> Downloading PDF for row 6, Title='Carbon Farming practices asses' ...\n",
      " -> Downloading PDF for row 7, Title='Optimizing cover crop practice' ...\n",
      " -> Downloading PDF for row 8, Title='Cumulative impact of cover cro' ...\n",
      " -> Downloading PDF for row 9, Title='Potential carbon sequestration' ...\n",
      " -> Downloading PDF for row 10, Title='Deep soil inventories reveal t' ...\n",
      " -> Downloading PDF for row 11, Title='Cover crops do increase soil o' ...\n",
      " -> Downloading PDF for row 12, Title='Climate mitigation potential o' ...\n",
      " -> Downloading PDF for row 13, Title='Soil Carbon Sequestration Impa' ...\n",
      " -> Downloading PDF for row 14, Title='The business case for carbon f' ...\n",
      " -> Downloading PDF for row 15, Title='Cover crops support the climat' ...\n",
      " -> Downloading PDF for row 16, Title='Soil carbon sequestration and ' ...\n",
      " -> Downloading PDF for row 17, Title='Optimizing Carbon Sequestratio' ...\n",
      " -> Downloading PDF for row 18, Title='Enhancing estimation of cover ' ...\n",
      " -> Downloading PDF for row 19, Title='What Is Regenerative Agricultu' ...\n",
      " -> Downloading PDF for row 20, Title='Cover Crop Contributions to Im' ...\n",
      " -> Downloading PDF for row 21, Title='Improving Human Diets and Welf' ...\n",
      " -> Downloading PDF for row 22, Title='Soil Moisture Conservation thr' ...\n",
      " -> Downloading PDF for row 23, Title='How Do Mixed Cover Crops (Whit' ...\n",
      " -> Downloading PDF for row 24, Title='Screening Cover Crops for Util' ...\n",
      " -> Downloading PDF for row 25, Title='The Role of Cover Crops toward' ...\n",
      " -> Downloading PDF for row 26, Title='Growing Cover Crops to Improve' ...\n",
      " -> Downloading PDF for row 27, Title='Carbon sequestration potential' ...\n",
      "[*] Dropping rows with no PDF found or no DOI...\n",
      "[*] Successfully retrieved PDFs for 14 articles via Unpaywall.\n",
      "[*] After dropping no-PDF rows, we have 14 rows left.\n",
      "[Step 8/11] Adding PDFs to Zotero, generating citations...\n",
      "[*] Zotero processing complete.\n",
      "[Step 9/11] Extracting text/tables via Azure Document Intelligence...\n",
      "   -> Analyzing PDF for row 0, file: Cover crop cultivation strategies in a Scandinavian context for climate change mitigation and biogas_4.pdf\n",
      "   -> Analyzing PDF for row 1, file: Cumulative impact of cover crops on soil carbon sequestration and profitability in a temperate humid_8.pdf\n",
      "   -> Analyzing PDF for row 2, file: Deep soil inventories reveal that impacts of cover crops and compost on soil carbon sequestration di_10.pdf\n",
      "   -> Analyzing PDF for row 3, file: The business case for carbon farming in the USA._14.pdf\n",
      "   -> Analyzing PDF for row 4, file: Cover crops support the climate change mitigation potential of agroecosystems._15.pdf\n",
      "   -> Analyzing PDF for row 5, file: Optimizing Carbon Sequestration Through Cover Cropping in Mediterranean Agroecosystems Synthesis of _17.pdf\n",
      "   -> Analyzing PDF for row 6, file: Enhancing estimation of cover crop biomass using field-based high-throughput phenotyping and machine_18.pdf\n",
      "   -> Analyzing PDF for row 7, file: What Is Regenerative Agriculture A Review of Scholar and Practitioner Definitions Based on Processes_19.pdf\n",
      "   -> Analyzing PDF for row 8, file: Cover Crop Contributions to Improve the Soil Nitrogen and Carbon Sequestration in Almond Orchards SW_20.pdf\n",
      "   -> Analyzing PDF for row 9, file: Improving Human Diets and Welfare through Using Herbivore-Based Foods 2. Environmental Consequences _21.pdf\n",
      "   -> Analyzing PDF for row 10, file: Soil Moisture Conservation through Crop Diversification and Related Ecosystem Services in a Blown-Sa_22.pdf\n",
      "   -> Analyzing PDF for row 11, file: How Do Mixed Cover Crops White Mustard + Oats Contribute to Labile Carbon Pools in an Organic Croppi_23.pdf\n",
      "   -> Analyzing PDF for row 12, file: Screening Cover Crops for Utilization in Irrigated Vineyards A Greenhouse Study on Species' Nitrogen_24.pdf\n",
      "   -> Analyzing PDF for row 13, file: Carbon sequestration potential of residues of different types of cover crops in olive groves under m_27.pdf\n",
      "[Step 10/11] Saving final DataFrame to Feather...\n",
      "[*] Results also saved to G:/ETL_OUT/ETL_output2.feather\n",
      "\n",
      "=== Pipeline complete. Check log for details. ===\n",
      "[Step 11/11] Saving final DataFrame to Feather...\n",
      "Loading DataFrame from G:/ETL_OUT/ETL_output2.feather ...\n",
      "DataFrame loaded successfully.\n",
      "Processing row 0 ...\n",
      "Row 0: Text chunked into 2 chunks.\n",
      "Row 0, Chunk 0: Sending prompt to OpenAI.\n",
      "2025-02-15 20:43:39,708 - ERROR - Could not parse JSON for row 0, chunk 0: Invalid control character at: line 7 column 242 (char 1200)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_46712\\3049562527.py\", line 264, in extract_additional_fields\n",
      "    new_data = json.loads(content)\n",
      "               ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "json.decoder.JSONDecodeError: Invalid control character at: line 7 column 242 (char 1200)\n",
      "Row 0, Chunk 0: JSON parsing failed.\n",
      "Row 0, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 0, Chunk 1: Successfully merged new data.\n",
      "Row 0: Extraction complete.\n",
      "Processing row 1 ...\n",
      "Row 1: Text chunked into 1 chunks.\n",
      "Row 1, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 1, Chunk 0: Successfully merged new data.\n",
      "Row 1: Extraction complete.\n",
      "Processing row 2 ...\n",
      "Row 2: Text chunked into 2 chunks.\n",
      "Row 2, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 2, Chunk 0: Successfully merged new data.\n",
      "Row 2, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 2, Chunk 1: Successfully merged new data.\n",
      "Row 2: Extraction complete.\n",
      "Processing row 3 ...\n",
      "Row 3: Text chunked into 2 chunks.\n",
      "Row 3, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 3, Chunk 0: Successfully merged new data.\n",
      "Row 3, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 3, Chunk 1: Successfully merged new data.\n",
      "Row 3: Extraction complete.\n",
      "Processing row 4 ...\n",
      "Row 4: Text chunked into 2 chunks.\n",
      "Row 4, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 4, Chunk 0: Successfully merged new data.\n",
      "Row 4, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 4, Chunk 1: Successfully merged new data.\n",
      "Row 4: Extraction complete.\n",
      "Processing row 5 ...\n",
      "Row 5: Text chunked into 3 chunks.\n",
      "Row 5, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 5, Chunk 0: Successfully merged new data.\n",
      "Row 5, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 5, Chunk 1: Successfully merged new data.\n",
      "Row 5, Chunk 2: Sending prompt to OpenAI.\n",
      "Row 5, Chunk 2: Successfully merged new data.\n",
      "Row 5: Extraction complete.\n",
      "Processing row 6 ...\n",
      "Row 6: Text chunked into 1 chunks.\n",
      "Row 6, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 6, Chunk 0: Successfully merged new data.\n",
      "Row 6: Extraction complete.\n",
      "Processing row 7 ...\n",
      "Row 7: Text chunked into 1 chunks.\n",
      "Row 7, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 7, Chunk 0: Successfully merged new data.\n",
      "Row 7: Extraction complete.\n",
      "Processing row 8 ...\n",
      "Row 8: Text chunked into 1 chunks.\n",
      "Row 8, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 8, Chunk 0: Successfully merged new data.\n",
      "Row 8: Extraction complete.\n",
      "Processing row 9 ...\n",
      "Row 9: Text chunked into 2 chunks.\n",
      "Row 9, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 9, Chunk 0: Successfully merged new data.\n",
      "Row 9, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 9, Chunk 1: Successfully merged new data.\n",
      "Row 9: Extraction complete.\n",
      "Processing row 10 ...\n",
      "Row 10: Text chunked into 1 chunks.\n",
      "Row 10, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 10, Chunk 0: Successfully merged new data.\n",
      "Row 10: Extraction complete.\n",
      "Processing row 11 ...\n",
      "Row 11: Text chunked into 1 chunks.\n",
      "Row 11, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 11, Chunk 0: Successfully merged new data.\n",
      "Row 11: Extraction complete.\n",
      "Processing row 12 ...\n",
      "Row 12: Text chunked into 2 chunks.\n",
      "Row 12, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 12, Chunk 0: Successfully merged new data.\n",
      "Row 12, Chunk 1: Sending prompt to OpenAI.\n",
      "Row 12, Chunk 1: Successfully merged new data.\n",
      "Row 12: Extraction complete.\n",
      "Processing row 13 ...\n",
      "Row 13: Text chunked into 1 chunks.\n",
      "Row 13, Chunk 0: Sending prompt to OpenAI.\n",
      "Row 13, Chunk 0: Successfully merged new data.\n",
      "Row 13: Extraction complete.\n",
      "Enriched DataFrame saved to G:/ETL_OUT/enriched_output.feather\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# 9) Main Pipeline\n",
    "# ------------------------------------------------------------------------------\n",
    "def pipeline():\n",
    "    print(\"\\n=== Starting ETL + RAG Pipeline ===\")\n",
    "    logging.info(\"Pipeline execution started.\")\n",
    "\n",
    "    # -----------------------\n",
    "    # STEP 1: Collect queries\n",
    "    # -----------------------\n",
    "    print(\"[Setp 1/11] Enter multiple queries, one per line. Press ENTER on a blank line to finish.\")\n",
    "    user_queries = []\n",
    "    while True:\n",
    "        line = input(\"Query (blank line to finish): \").strip()\n",
    "        if not line:\n",
    "            break\n",
    "        user_queries.append(line)\n",
    "\n",
    "    if not user_queries:\n",
    "        print(\"[!] No queries entered. Exiting.\")\n",
    "        return\n",
    "    # ------------------------------------------------\n",
    "    # STEP 2: Choose Citation Style (unchanged logic)\n",
    "    # ------------------------------------------------\n",
    "    print(\"[Step 2/10] Choose your citation style:\")\n",
    "    for k, style_info in CITATION_STYLES.items():\n",
    "        print(f\"   {k}) {style_info['name']}\", flush=True)\n",
    "    style_choice = input(\"Enter style number(e.g., 1/2/3/...): \").strip()\n",
    "    style_data = CITATION_STYLES.get(style_choice)\n",
    "    if not style_data:\n",
    "        # Fallback if user picks none\n",
    "        print(\"[!] Invalid style choice. Defaulting to 'APA 6th ed.'\")\n",
    "        style_data = CITATION_STYLES[\"2\"]  # e.g. APA 6th\n",
    "    chosen_style_path = style_data[\"path\"]\n",
    "    chosen_style_name = style_data[\"name\"]\n",
    "    print(f\"[*] You selected style: {chosen_style_name}\")    \n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # STEP 3: Choose data sources\n",
    "    # ------------------------------------------------    \n",
    "        \n",
    "    print(\"[Step 3/11] Choose your data source(s):\")\n",
    "    print(\"   1) PubMed only\")\n",
    "    print(\"   2) OpenAlex (PyAlex) only\")\n",
    "    print(\"   3) Both PubMed + PyAlex\")\n",
    "    choice = input(\"Enter choice (1/2/3): \").strip()\n",
    "    if choice not in (\"1\",\"2\",\"3\"):\n",
    "        print(\"[!] Invalid choice. Exiting.\")\n",
    "        logging.warning(\"User chose invalid source selection.\")\n",
    "        return    \n",
    "        \n",
    "    # ------------------------------------------------\n",
    "    # STEP 3.1: How many articles\n",
    "    # ------------------------------------------------\n",
    "    desired_str = input(\"How many articles per query (max)? [default=50] \").strip()\n",
    "    try:\n",
    "        desired_num = int(desired_str)\n",
    "        print(f\"[*] You asked for up to {desired_num} articles per source.\")\n",
    "    except ValueError:\n",
    "        desired_num = 50\n",
    "    print(f\"[*] Will retrieve up to {desired_num} articles per query for each source.\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Initialize storage for partial results\n",
    "    # -----------------------------------------------\n",
    "    from_pubmed_all = []\n",
    "    from_openalex_all = []\n",
    "\n",
    "    # OPTIONAL: We'll keep track of how many hits each query got from each source\n",
    "    pubmed_query_counts = {}\n",
    "    openalex_query_counts = {}\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # STEP 4: Loop over user queries\n",
    "    # ------------------------------------------------\n",
    "\n",
    "    for q in user_queries:\n",
    "        print(f\"[Step 4/11]\\n=== PROCESSING QUERY: '{q}' ===\")\n",
    "\n",
    "        # ~~~~~~~~~~~~ PubMed retrieval ~~~~~~~~~~~~~~~\n",
    "        if choice in (\"1\",\"3\"):  # PubMed\n",
    "            try:\n",
    "                pmid_list = pubmed_search_chunked(q, desired_count=desired_num, chunk_size=200)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error searching PubMed for '{q}': {e}\", exc_info=True)\n",
    "                print(f\"[!] PubMed query failed for '{q}': {e}\")\n",
    "                pmid_list = []\n",
    "                \n",
    "            if pmid_list:\n",
    "                #fetch metadata\n",
    "                try:\n",
    "                    pm_data = fetch_pubmed_metadata(pmid_list)\n",
    "                    pm_df = pd.DataFrame(pm_data)\n",
    "                    from_pubmed_all.append(pm_df)\n",
    "                    pubmed_query_counts[q] = len(pm_df)\n",
    "                    print(f\"     -> PubMed Returned {len(pm_df)} articles for '{q}.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error fetching PubMed metadata for '{q}': {e}\", exc_info=True)\n",
    "                    print(f\"[!] PubMed metadata fetch failed for '{q}': {e}\")\n",
    "                    pubmed_query_counts[q] = 0\n",
    "    \n",
    "            else:\n",
    "                pubmed_query_counts[q] = 0\n",
    "                \n",
    "            # ~~~~~~~~~~~~ OpenAlex retrieval ~~~~~~~~~~~~~~~\n",
    "            if choice in (\"2\",\"3\"):  # OpenAlex\n",
    "                try:\n",
    "                    oa_df = pyalex_search_works(q, desired_count=desired_num)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error searching OpenAlex for '{q}': {e}\", exc_info=True)\n",
    "                    print(f\"[!] OpenAlex query failed for '{q}': {e}\")\n",
    "                    oa_df = pd.DataFrame()\n",
    "                if not oa_df.empty:\n",
    "                    from_openalex_all.append(oa_df)\n",
    "                    openalex_query_counts[q] = len(oa_df)\n",
    "                    print(f\"    -> OpenAlex returned {len(oa_df)} articles for '{q}'.\")\n",
    "                else:\n",
    "                    openalex_query_counts[q] = 0\n",
    "                \n",
    "\n",
    "        # If no queries returned anything, just exit\n",
    "        if not from_pubmed_all and not from_openalex_all:\n",
    "            print(\"[!] No articles found across all queries. Exiting.\")\n",
    "            return\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Print a quick summary of how many hits we got\n",
    "    # --------------------------------------------\n",
    "    print(\"\\n=== Summary of Article Hits per Query ===\")\n",
    "    for q in user_queries:\n",
    "        pm_count = pubmed_query_counts.get(q, 0)\n",
    "        oa_count = openalex_query_counts.get(q, 0)\n",
    "        total_count = pm_count + oa_count\n",
    "        print(f\" - Query '{q}': PubMed={pm_count}, OpenAlex={oa_count}, combined={total_count}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Step 5: Merge & deduplicate\n",
    "    # -------------------------------------------------\n",
    "    print(\"[Step 5/11]\\nMerging all partial results and deduplicating...\")\n",
    "    combined_df = pd.DataFrame()\n",
    "    if from_pubmed_all:\n",
    "        combined_df = pd.concat(from_pubmed_all, ignore_index=True)\n",
    "    if from_openalex_all:\n",
    "        openalex_merged = pd.concat(from_openalex_all, ignore_index=True)\n",
    "        combined_df = pd.concat([combined_df, openalex_merged], ignore_index=True)\n",
    "        \n",
    "    combined_df = deduplicate_by_doi_title(combined_df)\n",
    "    print(f\"[*] After deduplication, total unique articles: {len(combined_df)}\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Step 7: PDF retrieval\n",
    "    # -------------------------------------------------\n",
    "    print(\"[Step 7/11] Attempting to download PDFs via Unpaywall...\")\n",
    "    combined_df[\"PDFPath\"] = None\n",
    "    combined_df[\"PDFStatus\"] = None\n",
    "\n",
    "    for i, row in combined_df.iterrows():\n",
    "        doi = row.get(\"DOI\",\"\")\n",
    "        title = row.get(\"Title\",\"\")\n",
    "\n",
    "        if not doi:\n",
    "            combined_df.at[i, \"PDFStatus\"] = \"No DOI\"\n",
    "            continue\n",
    "\n",
    "        title_val = row.get(\"Title\", \"\")\n",
    "        if not isinstance(title_val, str):\n",
    "            # convert None or float('nan') or anything else to an empty string\n",
    "            title_val = str(title_val) if title_val is not None else \"\"\n",
    "\n",
    "        print(f\" -> Downloading PDF for row {i}, Title='{title_val[:30]}' ...\")\n",
    "        pdf_content, ctype = get_pdf_from_unpaywall(doi)\n",
    "        if pdf_content:\n",
    "            safe_title = re.sub(r'[\\\\/*?:\"<>|()]+', '', title_val)\n",
    "            # Also limit length so it doesn't get too big\n",
    "            if len(safe_title) > 100:\n",
    "                safe_title = safe_title[:100]\n",
    "            filename = f\"{safe_title}_{i}.pdf\"\n",
    "    \n",
    "            save_path = os.path.join(PDF_SAVE_FOLDER, filename)\n",
    "            try:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(pdf_content)\n",
    "                combined_df.at[i, \"PDFPath\"] = save_path\n",
    "                combined_df.at[i, \"PDFStatus\"] = \"Saved\"\n",
    "                logging.info(f\"PDF saved for row {i}, Title={title[:30]}, path={save_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving PDF row={i}, Title={title[:30]}: {e}\")\n",
    "                combined_df.at[i, \"PDFPath\"] = None\n",
    "                combined_df.at[i, \"PDFStatus\"] = f\"Error saving PDF: {e}\"\n",
    "        else:\n",
    "            combined_df.at[i, \"PDFPath\"] = None\n",
    "            combined_df.at[i, \"PDFStatus\"] = \"No PDF found\"\n",
    "    print(\"[*] Dropping rows with no PDF found or no DOI...\")\n",
    "    pdf_saved_count = (combined_df[\"PDFStatus\"] == \"Saved\").sum()\n",
    "    print(f\"[*] Successfully retrieved PDFs for {pdf_saved_count} articles via Unpaywall.\")\n",
    "    combined_df = combined_df[~combined_df[\"PDFStatus\"].isin([\"No PDF found\", \"No DOI\"])].copy()\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"[*] After dropping no-PDF rows, we have {len(combined_df)} rows left.\")\n",
    "            \n",
    "    # ------------------------------------------------\n",
    "    # Step 8: Zotero Integration\n",
    "    # ------------------------------------------------\n",
    "    print(\"[Step 8/11] Adding PDFs to Zotero, generating citations...\")\n",
    "    combined_df[\"ZoteroKey\"] = \"\"\n",
    "    combined_df[\"InTextCitation\"] = \"\"\n",
    "    combined_df[\"FullCitation\"] = \"\"\n",
    "    for i, row in combined_df.iterrows():\n",
    "        pdf_path = row.get(\"PDFPath\", \"\")\n",
    "        if pdf_path and os.path.exists(pdf_path):\n",
    "            # [Enhancement] Pass all relevant metadata fields to add_pdf_to_zotero\n",
    "            metadata = {\n",
    "                \"DOI\": row.get(\"DOI\", \"\"),\n",
    "                \"Title\": row.get(\"Title\", \"\"),\n",
    "                \"Authors\": row.get(\"Authors\", []),  # [Enhancement]\n",
    "                \"Date\": row.get(\"Date\", \"\"),        # [Enhancement]\n",
    "                \"Journal\": row.get(\"Journal\", \"\"),  # [Enhancement]\n",
    "                \"Volume\": row.get(\"Volume\", \"\"),    # [Enhancement]\n",
    "                \"Issue\": row.get(\"Issue\", \"\"),      # [Enhancement]\n",
    "                \"Pages\": row.get(\"Pages\", \"\"),      # [Enhancement]\n",
    "            }\n",
    "            item_key = add_pdf_to_zotero(pdf_path, metadata)\n",
    "            if item_key:\n",
    "                combined_df.at[i, \"ZoteroKey\"] = item_key\n",
    "                intext, fullcite = get_citation_csl_json(item_key, style=chosen_style_path)\n",
    "                if intext and fullcite:\n",
    "                    combined_df.at[i, \"InTextCitation\"] = intext\n",
    "                    combined_df.at[i, \"FullCitation\"] = fullcite\n",
    "                else:\n",
    "                    combined_df.at[i, \"InTextCitation\"] = \"N/A\"\n",
    "                    combined_df.at[i, \"FullCitation\"] = \"N/A\"\n",
    "            else:\n",
    "                combined_df.at[i, \"ZoteroKey\"] = \"Error in creation\"\n",
    "        else:\n",
    "            combined_df.at[i, \"ZoteroKey\"] = \"No PDF\"\n",
    "\n",
    "    print(\"[*] Zotero processing complete.\")        \n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # Step 9: Azure extraction\n",
    "    # ------------------------------------------------\n",
    "    print(\"[Step 9/11] Extracting text/tables via Azure Document Intelligence...\")\n",
    "    azure_extractor = AzurePDFExtractor(AZURE_ENDPOINT, AZURE_KEY)\n",
    "\n",
    "    combined_df[\"FullText\"] = \"\"\n",
    "    combined_df[\"TablesJson\"] = \"\"\n",
    "    combined_df[\"TokenCount\"] = 0\n",
    "\n",
    "    for i, row in combined_df.iterrows():\n",
    "        pdf_path = row[\"PDFPath\"]\n",
    "        if not pdf_path or not os.path.exists(pdf_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"   -> Analyzing PDF for row {i}, file: {os.path.basename(pdf_path)}\")\n",
    "            full_text, table_dfs = azure_extractor.extract_text_and_tables(pdf_path)\n",
    "            combined_df.at[i, \"FullText\"] = full_text\n",
    "            tcount = count_tokens(full_text)\n",
    "            combined_df.at[i, \"TokenCount\"] = tcount\n",
    "\n",
    "            # Convert tables to JSON\n",
    "            table_list = [df_table.to_dict(orient=\"records\") for df_table in table_dfs]\n",
    "            combined_df.at[i, \"TablesJson\"] = json.dumps(table_list, ensure_ascii=False)\n",
    "            \n",
    "        except HttpResponseError as e:\n",
    "            logging.error(f\"Azure error row={i}, PDF={pdf_path}: {e}\")\n",
    "            combined_df.at[i, \"FullText\"] = \"ANALYSIS_ERROR\"\n",
    "            combined_df.at[i, \"TablesJson\"] = \"ANALYSIS_ERROR\"\n",
    "            print(f\"       -> [!] Azure Document Intelligence error for {pdf_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error row={i}, PDF={pdf_path}: {e}\")\n",
    "            combined_df.at[i, \"FullText\"] = \"ANALYSIS_ERROR\"\n",
    "            combined_df.at[i, \"TablesJson\"] = \"ANALYSIS_ERROR\"\n",
    "            print(f\"       -> [!] Unexpected error analyzing {pdf_path}: {e}\")\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Step 10: Save final\n",
    "    # ------------------------------------------------\n",
    "    print(\"[Step 10/11] Saving final DataFrame to Feather...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert citation columns to string to avoid feather conversion issues\n",
    "        combined_df[\"InTextCitation\"] = combined_df[\"InTextCitation\"].astype(str)\n",
    "        combined_df[\"FullCitation\"] = combined_df[\"FullCitation\"].astype(str)\n",
    "        combined_df.to_feather(OUTPUT_FEATHER)\n",
    "        print(f\"[*] Results also saved to {OUTPUT_FEATHER}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving Feather: {e}\")\n",
    "        print(f\"[!] Error saving Feather file: {e}\")\n",
    "\n",
    "    print(\"\\n=== Pipeline complete. Check log for details. ===\")\n",
    "    logging.info(\"Pipeline execution finished.\")\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_df = pipeline()\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------\n",
    "    # Step 11: field extraction\n",
    "    # ------------------------------------------------\n",
    "    # 2) Now call the new extraction function on the Feather file\n",
    "#    which will parse text/tables, chunk, LLM extraction, store new fields + \"AllExtracted\"\n",
    "    print(\"[Step 11/11] Saving final DataFrame to Feather...\")\n",
    "    enriched_df = extract_additional_fields(feather_path=OUTPUT_FEATHER, output_path=ENRICHED_FEATHER)\n",
    "\n",
    "    # \"enriched_output.feather\" now includes columns like \"StudyType\", \"Scenario\", \"Metrics\", etc.,\n",
    "    # plus an \"AllExtracted\" column with the entire JSON record per row.\n",
    "    print(\"All done!\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
